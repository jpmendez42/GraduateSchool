{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1956a6ed",
   "metadata": {},
   "source": [
    "# Final Project Script for Loading Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96842fa2",
   "metadata": {},
   "source": [
    "### API for GBIF: https://pygbif.readthedocs.io/en/latest/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1c8bda",
   "metadata": {},
   "source": [
    "## Get imports for pygbif library and image loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52383411",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pygbif import species as species\n",
    "from pygbif import occurrences as occurrences\n",
    "from IPython.display import Image, display\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d65054",
   "metadata": {},
   "source": [
    "## For our usecase, we are looking at plants around Knoxville, so we are going to use Knoxville's coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "efe06ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "latitude = 35.964668\n",
    "longitude = -83.926453\n",
    "\n",
    "#We need to make the geometry a polygon, so we're going to make a square out of these coords.\n",
    "\n",
    "min_lat = latitude - 0.09\n",
    "max_lat = latitude + 0.09\n",
    "min_lon = longitude - 0.12\n",
    "max_lon = longitude + 0.12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34c43d6",
   "metadata": {},
   "source": [
    "## Search for occurrences within the box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b9c23bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = occurrences.search(\n",
    "    kingdomKey=6,\n",
    "    decimalLatitude=f\"{min_lat}, {max_lat}\",\n",
    "    decimalLongitude=f\"{min_lon},{max_lon}\",\n",
    "    facet='speciesKey',\n",
    "    limit = 0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6cc5ee",
   "metadata": {},
   "source": [
    "## Generate results (DEPRECIATED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14d341fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for record in results['results']:\n",
    "    count = 0\n",
    "    species_name = record.get('species') or record.get('scientificName')\n",
    "    print(f\"Species: {species_name}\")\n",
    "    if 'media' in record:\n",
    "        for media in record['media']:\n",
    "            url = media.get('identifier')\n",
    "            #try:\n",
    "            #    response = requests.get(url)\n",
    "            #    display(Image(data=response.content))\n",
    "            #except:\n",
    "                #print(\"Could not Print\")\n",
    "    #print(count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad62ac06",
   "metadata": {},
   "outputs": [],
   "source": [
    "species_keys = [int(f['name']) for f in results['facets'][0]['counts']] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc6f6f30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SpeciesKey 7902463 has 602 images.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"https://inaturalist-open-data.s3.amazonaws.com/photos/461253020/original.jpg\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"https://inaturalist-open-data.s3.amazonaws.com/photos/461253720/original.jpg\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"https://inaturalist-open-data.s3.amazonaws.com/photos/461254609/original.jpg\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"https://inaturalist-open-data.s3.amazonaws.com/photos/461309374/original.jpg\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"https://inaturalist-open-data.s3.amazonaws.com/photos/461309022/original.jpg\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "images = {}\n",
    "\n",
    "for key in species_keys:\n",
    "    img_result = occurrences.search(\n",
    "        speciesKey = key,\n",
    "        mediatype='StillImage',\n",
    "        limit=300\n",
    "    )\n",
    "    urls = []\n",
    "    for rec in img_result['results']:\n",
    "        for m in rec.get('media', []):\n",
    "            if m.get('identifier'):\n",
    "                urls.append(m['identifier'])\n",
    "    images[key] = urls\n",
    "\n",
    "# Example: how many images for the first species?\n",
    "first = species_keys[0]\n",
    "print(f\"SpeciesKey {first} has {len(images[first])} images.\")\n",
    "for url in images[first][:5]:\n",
    "    display(Image(url=url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d54f455f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jacob/CodeSpace/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-05-04 17:52:26.186521: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-05-04 17:52:26.377915: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1746395546.453098    9435 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1746395546.475982    9435 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-05-04 17:52:26.652806: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import torch\n",
    "\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "def get_clip_embedding(img_path):\n",
    "    image = Image.open(img_path).convert(\"RGB\")\n",
    "    inputs = processor(images=image, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model.get_image_features(**inputs)\n",
    "    return outputs[0]  # shape: [512]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ecfa1d8f",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "type object 'Image' has no attribute 'open'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mget_clip_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[8], line 8\u001b[0m, in \u001b[0;36mget_clip_embedding\u001b[0;34m(img_path)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_clip_embedding\u001b[39m(img_path):\n\u001b[0;32m----> 8\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m(img_path)\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m processor(images\u001b[38;5;241m=\u001b[39mimage, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "\u001b[0;31mAttributeError\u001b[0m: type object 'Image' has no attribute 'open'"
     ]
    }
   ],
   "source": [
    "get_clip_embedding(url)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
